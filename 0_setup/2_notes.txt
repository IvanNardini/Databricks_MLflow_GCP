
### References:

  - https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation
  - https://github.com/mlflow/mlflow/tree/master/examples/flower_classifier
  - https://docs.azuredatabricks.net/_static/notebooks/mleap-model-export-demo-python.html
  - https://www.mlflow.org/docs/latest/tracking.html
  - https://docs.azuredatabricks.net/_static/notebooks/mleap-model-export-demo-scala.html
  - https://mleap-docs.combust.ml/mleap-runtime/transform-leap-frame.html
  - https://github.com/combust/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree
  - https://github.com/combust/mleap/issues/172
  - https://cloud.google.com/dataproc/docs/tutorials/spark-scala

### Checks: 

# Based on the last experiment...
# download_artifacts(run_id, path, dst_path=None)
# MlflowClient().download_artifacts(runID, '/artifacts/pyspark-multi-linear-model')

# %%bash
# spark-shell --packages ml.combust.mleap:mleap-spark_2.11:0.15.0

# Create Working Directory

# MLpackagePath = "/FileStore/ModelProjects/Boston_ML"
# dbutils.fs.rm(MLpackagePath, True)
# dbutils.fs.mkdirs(MLpackagePath)
# dbutils.fs.ls(MLpackagePath)

# # Prepare the environment
# # Copy data to score
# dbutils.fs.cp("dbfs:/data/boston_house_prices.csv", "dbfs:/FileStore/ModelProjects/Boston_ML")
# # Copy model to consume for scoring
# dbutils.fs.cp("dbfs:/example/lrModel.zip","dbfs:/FileStore/ModelProjects/Boston_ML")
# # Check the content
# dbutils.fs.ls(MLpackagePath)

# # Copy model to consume for scoring
# dbutils.fs.cp("dbfs:/example/lrModel.zip","/tmp")
# # Check the content
# dbutils.fs.ls("/tmp")

## 3.  Create a Spark Scala job to run on Cloud Dataproc for deploying the model in batch

dbutils.fs.put(f"{MLpackagePath}/score.py",
"""
#!/usr/bin/python

import numpy as np
import pandas as pd
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType
from pyspark.ml import PipelineModel
from pyspark.ml.evaluation import RegressionEvaluator


import os
import sys
import argparse
import tempfile
import warnings

# Read Data

# def read_data_csv(spark, inputPath_CSV):
  
#   '''
#   Function to load data in the Spark Session 
#   :param spark: spark session 
#   :param inputPath: path to get the data 
#   :return: df
#   '''
  
#   print('trying to read the data...')
  
#   try:
#     # define the schema
#     schema = StructType([
#       StructField('crim',DoubleType(),True),
#       StructField('zn',DoubleType(),True),
#       StructField('indus',DoubleType(),True),
#       StructField('chas',IntegerType(),True),
#       StructField('nox',DoubleType(),True),
#       StructField('rm',DoubleType(),True),
#       StructField('age',DoubleType(),True),
#       StructField('dis',DoubleType(),True),
#       StructField('rad',IntegerType(),True),
#       StructField('tax',IntegerType(),True),
#       StructField('ptratio',DoubleType(),True),
#       StructField('b',DoubleType(),True),
#       StructField('lstat',DoubleType(),True),
#       StructField('medv',DoubleType(),True)]
#     )

#     df = (spark.read
#           .option("HEADER", True)
#           .schema(schema)
#           .csv(datapath))
    
#   except ValueError:
#     print('At least, one variable format is wrong! \
#     Please check the data')
      
#   else:
#     print('Data to score have been read successfully!')
#     return df
  
# #Preprocessing

# def preprocessing(df):
  
#   '''
#   Function to preprocess data 
#   :param df: A pyspark DataFrame 
#   :return: abt_to_score
#   '''
  
#   print('Data preprocessing...')
  
#   features = df.schema.names[:-1]
#   assembler_features = VectorAssembler(inputCols=features, outputCol="features")
#   abt_to_score = assembler_features.transform(df)
#   return abt_to_score

# #Scoring
# def score_data(abt_to_score, modelPath):
  
#   '''
#   Function to score data 
#   :param abt_to_score: A pyspark DataFrame to score
#   :param modelPath: The modelpath associated to .zip mleap flavor
#   :return: scoredData
#   '''
  
#   print('Scoring process starts...')
  
#   deserializedPipeline = PipelineModel.deserializeFromBundle("jar:file:{}".format(modelpath))
#   scoredData = deserializedPipeline.transform(abt_to_score)
#   return scoredData  
  
# def write_output_csv(scoredData, outputPath_CSV):
  
#   '''
#   Function to write predictions
#   :param scoredData: A pyspark DataFrame of predictions
#   :param outputPath: The path to write the ouput table
#   :return: scoredData
#   '''

#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)
#   return outputDf.toPandas().to_dict()
  
def main():

  parser = argparse.ArgumentParser(description='Score')
  
  parser.add_argument('-i', dest="inputpath_CSV",
                        required=True, help='Provide the input path of data to score')

  args = parser.parse_args()
  input_path_CSV = args.inputpath_CSV
  
#   #Create a Spark Session
#   spark = SparkSession.builder.appName('MyApp').config("spark.master", "local").getOrCreate()
  
  #Read data
  #read_data_csv(spark, input_path_CSV)
  
  
if __name__=="__main__":

  from pyspark import SparkContext
  from pyspark.sql import SQLContext

  try:
    conf = pyspark.SparkConf().setMaster("local").setAppName("My app")
    sc = SparkContext.getOrCreate(conf)
   # sqlContext = SQLContext.getOrCreate(sc)
#     sc = pyspark.SparkContext()
#     sc.setLogLevel("ERROR")
#     sqlContext = pyspark.sql.SQLContext(sc)
    print('Created a SparkContext')
      
  except ValueError:
      warnings.warn('SparkContext already exists in this scope')
      
  sys.exit(main())

""".strip(), True)

import subprocess
# errors in the created process are raised here too
try:
  output = subprocess.check_output(["python","/dbfs/FileStore/ModelProjects/Boston_ML/score.py", "-i", "/dbfs/FileStore/ModelProjects/Boston_ML/boston_house_prices.csv"], stderr=subprocess.STDOUT, universal_newlines=True)
except subprocess.CalledProcessError as exc:
    print("Status : FAIL", exc.returncode, exc.output)
else:
    print("Output: \n{}\n".format(output))


#!/usr/bin/python
import click


import numpy as np
import pandas as pd
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType
from pyspark.ml import PipelineModel
from pyspark.ml.evaluation import RegressionEvaluator


import os
import argparse
import tempfile
import warnings

@click.command()
@click.option("--inputPath_CSV", type=str, )

# Read Data

def read_data_csv(spark, inputPath_CSV):
  
  """
  Function to load data in the Spark Session 
  :param spark: spark session 
  :param inputPath: path to get the data 
  :return: df
  """
  
  print('trying to read the data...')
  
  try:
    # define the schema
    schema = StructType([
      StructField('crim',DoubleType(),True),
      StructField('zn',DoubleType(),True),
      StructField('indus',DoubleType(),True),
      StructField('chas',IntegerType(),True),
      StructField('nox',DoubleType(),True),
      StructField('rm',DoubleType(),True),
      StructField('age',DoubleType(),True),
      StructField('dis',DoubleType(),True),
      StructField('rad',IntegerType(),True),
      StructField('tax',IntegerType(),True),
      StructField('ptratio',DoubleType(),True),
      StructField('b',DoubleType(),True),
      StructField('lstat',DoubleType(),True),
      StructField('medv',DoubleType(),True)]
    )

    df = (spark.read
          .option("HEADER", True)
          .schema(schema)
          .csv(datapath))
    
  except ValueError:
    print('At least, one variable format is wrong! \
    Please check the data')
      
  else:
    print('Data to score have been read successfully!')
    return df
  
# #Preprocessing

# def preprocessing(df):
  
#   """
#   Function to preprocess data 
#   :param df: A pyspark DataFrame 
#   :return: abt_to_score
#   """
  
#   print('Data preprocessing...')
  
#   features = df.schema.names[:-1]
#   assembler_features = VectorAssembler(inputCols=features, outputCol="features")
#   abt_to_score = assembler_features.transform(df)
#   return abt_to_score

# #Scoring
# def score_data(abt_to_score, modelPath):
  
#   """
#   Function to score data 
#   :param abt_to_score: A pyspark DataFrame to score
#   :param modelPath: The modelpath associated to .zip mleap flavor
#   :return: scoredData
#   """
  
#   print('Scoring process starts...')
  
#   deserializedPipeline = PipelineModel.deserializeFromBundle("jar:file:{}".format(modelpath))
#   scoredData = deserializedPipeline.transform(abt_to_score)
#   return scoredData  
  
# def write_output_csv(scoredData, outputPath_CSV):
  
#   """
#   Function to write predictions
#   :param scoredData: A pyspark DataFrame of predictions
#   :param outputPath: The path to write the ouput table
#   :return: scoredData
#   """

#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)
#   return outputDf.toPandas().to_dict()
  
# def main():

#   parser = argparse.ArgumentParser(description='Score')

#   parser.add_argument('-s', dest="Spark_Session",
#                       help='Provide the name of Spark Session')
  
#   parser.add_argument('-i', dest="inputpath_CSV",
#                         required=True, help='Provide the input path of data to score')

#   args = parser.parse_args()
#   spark_session = args.Spark_session
#   input_path_CSV = args.Input_path_CSV
  
#   #Create a Spark Session
#   spark = SparkSession.builder.appName(spark_session).getOrCreate()
  
#   #Read data
#   read_data_csv(spark, inputPath_CSV)
  
# if __name__=="__main__":
#   sys.exit(main())

from click.testing import CliRunner

runner = CliRunner()
result1 = runner.invoke(read_data_csv, ['--datapath', '/data/boston_house_prices.csv'], catch_exceptions=True)

assert result1.exit_code == 0, "Code failed" # Check to see that it worked

print("Success!")

print(result1.output)

dbutils.fs.put(f"{MLpackagePath}/score.py", 

"""
#!/usr/bin/python

print('suca')

""".strip(), True)

import subprocess

# errors in the created process are raised here too
output = subprocess.check_output(["python","/dbfs/FileStore/ModelProjects/Boston_ML/score.py"], universal_newlines=True)

print(output)