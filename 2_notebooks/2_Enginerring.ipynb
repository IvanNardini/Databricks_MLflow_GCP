{"cells":[{"cell_type":"markdown","source":["## Packaging Champion ML Project for GCP deployment"],"metadata":{}},{"cell_type":"markdown","source":["Let's create a simple MLflow project programmatically with:\n\n1. Create a Working Dir\n\n2. Create score.py\n\n<!-- 3. Create the .sh to run the score.py\n\n\n2. Create the ML project:\n  - MLProject file\n  - Conda environment\n  - Basic machine learning script\n\n3. Create the scoring script\n4. Test the scoring script\n5. Create the entrypoint file:\n  - execute .sh (Create a Spark cluster, Install Mlflow, Run Batch Scoring Job based on score python code in cloud bucket) -->"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Create a working Dir"],"metadata":{}},{"cell_type":"code","source":["MLpackagePath = \"/FileStore/ModelProjects/Boston_ML\"\ndbutils.fs.rm(MLpackagePath, True)\ndbutils.fs.mkdirs(MLpackagePath)\ndbutils.fs.ls(MLpackagePath)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Prepare the environment\n# Copy data to score\ndbutils.fs.cp(\"dbfs:/data/boston_house_prices.csv\", \"dbfs:/FileStore/ModelProjects/Boston_ML\")\n# Copy model to consume for scoring\ndbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"dbfs:/FileStore/ModelProjects/Boston_ML\")\n# Check the content\ndbutils.fs.ls(MLpackagePath)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## 2. Create score.py"],"metadata":{}},{"cell_type":"code","source":["#!/usr/bin/python\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport sys\nimport argparse\nimport tempfile\nimport warnings\n\n# Read Data\n\ndef read_data_csv(spark, inputPath_CSV):\n  \n  \"\"\"\n  Function to load data in the Spark Session \n  :param spark: spark session \n  :param inputPath: path to get the data \n  :return: df\n  \"\"\"\n  \n  print('trying to read the data...')\n  \n  try:\n    # define the schema\n    schema = StructType([\n      StructField('crim',DoubleType(),True),\n      StructField('zn',DoubleType(),True),\n      StructField('indus',DoubleType(),True),\n      StructField('chas',IntegerType(),True),\n      StructField('nox',DoubleType(),True),\n      StructField('rm',DoubleType(),True),\n      StructField('age',DoubleType(),True),\n      StructField('dis',DoubleType(),True),\n      StructField('rad',IntegerType(),True),\n      StructField('tax',IntegerType(),True),\n      StructField('ptratio',DoubleType(),True),\n      StructField('b',DoubleType(),True),\n      StructField('lstat',DoubleType(),True),\n      StructField('medv',DoubleType(),True)]\n    )\n\n    df = (spark.read\n          .option(\"HEADER\", True)\n          .schema(schema)\n          .csv(datapath))\n    \n  except ValueError:\n    print('At least, one variable format is wrong! \\\n    Please check the data')\n      \n  else:\n    print('Data to score have been read successfully!')\n    return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   \"\"\"\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   \"\"\"\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   \"\"\"\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   \"\"\"\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   \"\"\"\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   \"\"\"\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \ndef main():\n\n  parser = argparse.ArgumentParser(description='Score')\n  \n  parser.add_argument('-i', dest=\"inputpath_CSV\",\n                        required=True, help='Provide the input path of data to score')\n\n  args = parser.parse_args()\n  input_path_CSV = args.inputpath_CSV\n  \n  #Create a Spark Session\n  spark = SparkSession.builder.appName('MyApp').config(\"spark.master\", \"local\").getOrCreate()\n  \n  #Read data\n  read_data_csv(spark, input_path_CSV)\n  \n  \nif __name__==\"__main__\":\n  sys.exit(main())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import subprocess\n\n# errors in the created process are raised here too\noutput = subprocess.check_output([\"python\", \"--version\"], universal_newlines=True)\n\nprint(output)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# errors in the created process are raised here too\ntry:\n  output = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\", \"-i\", \"/dbfs/FileStore/ModelProjects/Boston_ML/boston_house_prices.csv\"], stderr=subprocess.STDOUT, universal_newlines=True)\nexcept subprocess.CalledProcessError as exc:\n    print(\"Status : FAIL\", exc.returncode, exc.output)\nelse:\n    print(\"Output: \\n{}\\n\".format(output))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Test score.py"],"metadata":{}},{"cell_type":"code","source":["#!/usr/bin/python\nimport click\n\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport argparse\nimport tempfile\nimport warnings\n\n@click.command()\n@click.option(\"--inputPath_CSV\", type=str, )\n\n# Read Data\n\ndef read_data_csv(spark, inputPath_CSV):\n  \n  \"\"\"\n  Function to load data in the Spark Session \n  :param spark: spark session \n  :param inputPath: path to get the data \n  :return: df\n  \"\"\"\n  \n  print('trying to read the data...')\n  \n  try:\n    # define the schema\n    schema = StructType([\n      StructField('crim',DoubleType(),True),\n      StructField('zn',DoubleType(),True),\n      StructField('indus',DoubleType(),True),\n      StructField('chas',IntegerType(),True),\n      StructField('nox',DoubleType(),True),\n      StructField('rm',DoubleType(),True),\n      StructField('age',DoubleType(),True),\n      StructField('dis',DoubleType(),True),\n      StructField('rad',IntegerType(),True),\n      StructField('tax',IntegerType(),True),\n      StructField('ptratio',DoubleType(),True),\n      StructField('b',DoubleType(),True),\n      StructField('lstat',DoubleType(),True),\n      StructField('medv',DoubleType(),True)]\n    )\n\n    df = (spark.read\n          .option(\"HEADER\", True)\n          .schema(schema)\n          .csv(datapath))\n    \n  except ValueError:\n    print('At least, one variable format is wrong! \\\n    Please check the data')\n      \n  else:\n    print('Data to score have been read successfully!')\n    return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   \"\"\"\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   \"\"\"\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   \"\"\"\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   \"\"\"\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   \"\"\"\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   \"\"\"\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \n# def main():\n\n#   parser = argparse.ArgumentParser(description='Score')\n\n#   parser.add_argument('-s', dest=\"Spark_Session\",\n#                       help='Provide the name of Spark Session')\n  \n#   parser.add_argument('-i', dest=\"inputpath_CSV\",\n#                         required=True, help='Provide the input path of data to score')\n\n#   args = parser.parse_args()\n#   spark_session = args.Spark_session\n#   input_path_CSV = args.Input_path_CSV\n  \n#   #Create a Spark Session\n#   spark = SparkSession.builder.appName(spark_session).getOrCreate()\n  \n#   #Read data\n#   read_data_csv(spark, inputPath_CSV)\n  \n  \n# if __name__==\"__main__\":\n#   sys.exit(main())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from click.testing import CliRunner\n\nrunner = CliRunner()\nresult1 = runner.invoke(read_data_csv, ['--datapath', '/data/boston_house_prices.csv'], catch_exceptions=True)\n\nassert result1.exit_code == 0, \"Code failed\" # Check to see that it worked\n\nprint(\"Success!\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["print(result1.output)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["dbutils.fs.put(f\"{MLpackagePath}/score.py\",\n\"\"\"\n#!/usr/bin/python\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport sys\nimport argparse\nimport tempfile\nimport warnings\n\n# Read Data\n\ndef read_data_csv(spark, inputPath_CSV):\n  \n  '''\n  Function to load data in the Spark Session \n  :param spark: spark session \n  :param inputPath: path to get the data \n  :return: df\n  '''\n  \n  print('trying to read the data...')\n  \n  try:\n    # define the schema\n    schema = StructType([\n      StructField('crim',DoubleType(),True),\n      StructField('zn',DoubleType(),True),\n      StructField('indus',DoubleType(),True),\n      StructField('chas',IntegerType(),True),\n      StructField('nox',DoubleType(),True),\n      StructField('rm',DoubleType(),True),\n      StructField('age',DoubleType(),True),\n      StructField('dis',DoubleType(),True),\n      StructField('rad',IntegerType(),True),\n      StructField('tax',IntegerType(),True),\n      StructField('ptratio',DoubleType(),True),\n      StructField('b',DoubleType(),True),\n      StructField('lstat',DoubleType(),True),\n      StructField('medv',DoubleType(),True)]\n    )\n\n    df = (spark.read\n          .option(\"HEADER\", True)\n          .schema(schema)\n          .csv(datapath))\n    \n  except ValueError:\n    print('At least, one variable format is wrong! \\\n    Please check the data')\n      \n  else:\n    print('Data to score have been read successfully!')\n    return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   '''\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   '''\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   '''\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   '''\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   '''\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   '''\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \ndef main():\n\n  parser = argparse.ArgumentParser(description='Score')\n  \n  parser.add_argument('-i', dest=\"inputpath_CSV\",\n                        required=True, help='Provide the input path of data to score')\n\n  args = parser.parse_args()\n  input_path_CSV = args.inputpath_CSV\n  \n  #Create a Spark Session\n  spark = SparkSession.builder.appName('MyApp').config(\"spark.master\", \"local\").getOrCreate()\n  \n  #Read data\n  read_data_csv(spark, input_path_CSV)\n  \nif __name__==\"__main__\":\n  sys.exit(main())\n\n\"\"\".strip(), True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dbutils.fs.put(f\"{MLpackagePath}/score.py\", \n\n\"\"\"\n#!/usr/bin/python\n\nprint('suca')\n\n\"\"\".strip(), True)\n               "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["import subprocess\n\n# errors in the created process are raised here too\noutput = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\"], universal_newlines=True)\n\nprint(output)"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"2_Enginerring","notebookId":3075853380345907},"nbformat":4,"nbformat_minor":0}
