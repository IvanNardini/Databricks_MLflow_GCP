{"cells":[{"cell_type":"markdown","source":["## Packaging Champion Model (Mlean Flavor) for GCP deployment\n\n### References\n\n- https://docs.azuredatabricks.net/_static/notebooks/mleap-model-export-demo-scala.html\n-  https://cloud.google.com/dataproc/docs/tutorials/spark-scala"],"metadata":{}},{"cell_type":"markdown","source":["Let's create a simple MLflow project programmatically with:\n\n1. Create a Working Dir\n\n2. Create a scala job\n\n2. Create score.py\n\n<!-- 3. Create the .sh to run the score.py\n\n\n2. Create the ML project:\n  - MLProject file\n  - Conda environment\n  - Basic machine learning script\n\n3. Create the scoring script\n4. Test the scoring script\n5. Create the entrypoint file:\n  - execute .sh (Create a Spark cluster, Install Mlflow, Run Batch Scoring Job based on score python code in cloud bucket) -->"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Create a working Dir"],"metadata":{}},{"cell_type":"code","source":["MLpackagePath = \"/FileStore/ModelProjects/Boston_ML\"\ndbutils.fs.rm(MLpackagePath, True)\ndbutils.fs.mkdirs(MLpackagePath)\ndbutils.fs.ls(MLpackagePath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: []</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Prepare the environment\n# Copy data to score\ndbutils.fs.cp(\"dbfs:/data/boston_house_prices.csv\", \"dbfs:/FileStore/ModelProjects/Boston_ML\")\n# Copy model to consume for scoring\ndbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"dbfs:/FileStore/ModelProjects/Boston_ML\")\n# Check the content\ndbutils.fs.ls(MLpackagePath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [FileInfo(path=&#39;dbfs:/FileStore/ModelProjects/Boston_ML/boston_house_prices.csv&#39;, name=&#39;boston_house_prices.csv&#39;, size=35735),\n FileInfo(path=&#39;dbfs:/FileStore/ModelProjects/Boston_ML/lrModel.zip&#39;, name=&#39;lrModel.zip&#39;, size=1103)]</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## 2. Download the folder programmatically"],"metadata":{}},{"cell_type":"code","source":["# Copy model to consume for scoring\ndbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"/tmp\")\n# Check the content\ndbutils.fs.ls(\"/tmp\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[44]: [FileInfo(path=&#39;dbfs:/tmp/Boston_ML/&#39;, name=&#39;Boston_ML/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/tmp/boston_house_prices.csv&#39;, name=&#39;boston_house_prices.csv&#39;, size=35735),\n FileInfo(path=&#39;dbfs:/tmp/hive/&#39;, name=&#39;hive/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/tmp/lrModel.zip&#39;, name=&#39;lrModel.zip&#39;, size=1103),\n FileInfo(path=&#39;dbfs:/tmp/mleap_python_model_export/&#39;, name=&#39;mleap_python_model_export/&#39;, size=0)]</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## 2. Create score.py job"],"metadata":{}},{"cell_type":"code","source":["#!/usr/bin/python\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport sys\nimport argparse\nimport tempfile\nimport warnings\n\n# Read Data\n\n# def read_data_csv(spark, inputPath_CSV):\n  \n#   \"\"\"\n#   Function to load data in the Spark Session \n#   :param spark: spark session \n#   :param inputPath: path to get the data \n#   :return: df\n#   \"\"\"\n  \n#   print('trying to read the data...')\n  \n#   try:\n#     # define the schema\n#     schema = StructType([\n#       StructField('crim',DoubleType(),True),\n#       StructField('zn',DoubleType(),True),\n#       StructField('indus',DoubleType(),True),\n#       StructField('chas',IntegerType(),True),\n#       StructField('nox',DoubleType(),True),\n#       StructField('rm',DoubleType(),True),\n#       StructField('age',DoubleType(),True),\n#       StructField('dis',DoubleType(),True),\n#       StructField('rad',IntegerType(),True),\n#       StructField('tax',IntegerType(),True),\n#       StructField('ptratio',DoubleType(),True),\n#       StructField('b',DoubleType(),True),\n#       StructField('lstat',DoubleType(),True),\n#       StructField('medv',DoubleType(),True)]\n#     )\n\n#     df = (spark.read\n#           .option(\"HEADER\", True)\n#           .schema(schema)\n#           .csv(datapath))\n    \n#   except ValueError:\n#     print('At least, one variable format is wrong! \\\n#     Please check the data')\n      \n#   else:\n#     print('Data to score have been read successfully!')\n#     return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   \"\"\"\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   \"\"\"\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   \"\"\"\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   \"\"\"\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   \"\"\"\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   \"\"\"\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \ndef main():\n\n  parser = argparse.ArgumentParser(description='Score')\n  \n  parser.add_argument('-i', dest=\"inputpath_CSV\",\n                        required=True, help='Provide the input path of data to score')\n\n  args = parser.parse_args()\n  input_path_CSV = args.inputpath_CSV\n  \n  try:\n    conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"My app\")\n    sc = SparkContext.getOrCreate(conf)\n    sqlContext = SQLContext.getOrCreate(sc)\n#     sc = pyspark.SparkContext()\n#     sc.setLogLevel(\"ERROR\")\n#     sqlContext = pyspark.sql.SQLContext(sc)\n    print('Created a SparkContext')\n      \n  except ValueError:\n      warnings.warn('SparkContext already exists in this scope')\n  \n#   #Create a Spark Session\n#   spark = SparkSession.builder.appName('MyApp').config(\"spark.master\", \"local\").getOrCreate()\n  \n  #Read data\n  read_data_csv(spark, input_path_CSV)\n  \n  \nif __name__==\"__main__\":\n  sys.exit(main())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">An exception has occurred, use %tb to see the full traceback.\n\n<span class=\"ansi-red-fg\">SystemExit</span><span class=\"ansi-red-fg\">:</span> 2\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## 3.  Create a Spark Scala job to run on Cloud Dataproc for deploying the model in batch"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.put(f\"{MLpackagePath}/score.py\",\n\"\"\"\n#!/usr/bin/python\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport sys\nimport argparse\nimport tempfile\nimport warnings\n\n# Read Data\n\n# def read_data_csv(spark, inputPath_CSV):\n  \n#   '''\n#   Function to load data in the Spark Session \n#   :param spark: spark session \n#   :param inputPath: path to get the data \n#   :return: df\n#   '''\n  \n#   print('trying to read the data...')\n  \n#   try:\n#     # define the schema\n#     schema = StructType([\n#       StructField('crim',DoubleType(),True),\n#       StructField('zn',DoubleType(),True),\n#       StructField('indus',DoubleType(),True),\n#       StructField('chas',IntegerType(),True),\n#       StructField('nox',DoubleType(),True),\n#       StructField('rm',DoubleType(),True),\n#       StructField('age',DoubleType(),True),\n#       StructField('dis',DoubleType(),True),\n#       StructField('rad',IntegerType(),True),\n#       StructField('tax',IntegerType(),True),\n#       StructField('ptratio',DoubleType(),True),\n#       StructField('b',DoubleType(),True),\n#       StructField('lstat',DoubleType(),True),\n#       StructField('medv',DoubleType(),True)]\n#     )\n\n#     df = (spark.read\n#           .option(\"HEADER\", True)\n#           .schema(schema)\n#           .csv(datapath))\n    \n#   except ValueError:\n#     print('At least, one variable format is wrong! \\\n#     Please check the data')\n      \n#   else:\n#     print('Data to score have been read successfully!')\n#     return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   '''\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   '''\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   '''\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   '''\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   '''\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   '''\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \ndef main():\n\n  parser = argparse.ArgumentParser(description='Score')\n  \n  parser.add_argument('-i', dest=\"inputpath_CSV\",\n                        required=True, help='Provide the input path of data to score')\n\n  args = parser.parse_args()\n  input_path_CSV = args.inputpath_CSV\n  \n#   #Create a Spark Session\n#   spark = SparkSession.builder.appName('MyApp').config(\"spark.master\", \"local\").getOrCreate()\n  \n  #Read data\n  #read_data_csv(spark, input_path_CSV)\n  \n  \nif __name__==\"__main__\":\n\n  from pyspark import SparkContext\n  from pyspark.sql import SQLContext\n\n  try:\n    conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"My app\")\n    sc = SparkContext.getOrCreate(conf)\n   # sqlContext = SQLContext.getOrCreate(sc)\n#     sc = pyspark.SparkContext()\n#     sc.setLogLevel(\"ERROR\")\n#     sqlContext = pyspark.sql.SQLContext(sc)\n    print('Created a SparkContext')\n      \n  except ValueError:\n      warnings.warn('SparkContext already exists in this scope')\n      \n  sys.exit(main())\n\n\"\"\".strip(), True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 3883 bytes.\nOut[6]: True</div>"]}}],"execution_count":11},{"cell_type":"code","source":["import subprocess\n# errors in the created process are raised here too\ntry:\n  output = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\", \"-i\", \"/dbfs/FileStore/ModelProjects/Boston_ML/boston_house_prices.csv\"], stderr=subprocess.STDOUT, universal_newlines=True)\nexcept subprocess.CalledProcessError as exc:\n    print(\"Status : FAIL\", exc.returncode, exc.output)\nelse:\n    print(\"Output: \\n{}\\n\".format(output))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Status : FAIL 1 Traceback (most recent call last):\n  File &#34;/dbfs/FileStore/ModelProjects/Boston_ML/score.py&#34;, line 131, in &lt;module&gt;\n    sc = SparkContext.getOrCreate(conf)\n  File &#34;/databricks/spark/python/pyspark/context.py&#34;, line 372, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &#34;/databricks/spark/python/pyspark/context.py&#34;, line 136, in __init__\n    conf, jsc, profiler_cls)\n  File &#34;/databricks/spark/python/pyspark/context.py&#34;, line 198, in _do_init\n    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n  File &#34;/databricks/spark/python/pyspark/context.py&#34;, line 311, in _initialize_context\n    return self._jvm.JavaSparkContext(jconf)\n  File &#34;/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py&#34;, line 1525, in __call__\n  File &#34;/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py&#34;, line 328, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: In Databricks, developers should utilize the shared SparkContext instead of creating one using the constructor. In Scala and Python notebooks, the shared context can be accessed as sc. When running a job, you can access the shared context by calling SparkContext.getOrCreate(). The other SparkContext was created at:\nCallSite(SparkContext at DatabricksILoop.scala:344,org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:90)\ncom.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:344)\ncom.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:344)\ncom.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)\ncom.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:343)\ncom.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:274)\ncom.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)\ncom.databricks.backend.daemon.driver.DriverCorral.&lt;init&gt;(DriverCorral.scala:212)\ncom.databricks.backend.daemon.driver.DriverDaemon.&lt;init&gt;(DriverDaemon.scala:35)\ncom.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:173)\ncom.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:178)\ncom.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:94)\ncom.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:93)\ncom.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:93)\ncom.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:287)\ncom.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:428)\ncom.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\ncom.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\ncom.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:63))\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2715)\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2709)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2709)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2810)\n\tat org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:99)\n\tat org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:250)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Test score.py"],"metadata":{}},{"cell_type":"code","source":["#!/usr/bin/python\nimport click\n\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport argparse\nimport tempfile\nimport warnings\n\n@click.command()\n@click.option(\"--inputPath_CSV\", type=str, )\n\n# Read Data\n\ndef read_data_csv(spark, inputPath_CSV):\n  \n  \"\"\"\n  Function to load data in the Spark Session \n  :param spark: spark session \n  :param inputPath: path to get the data \n  :return: df\n  \"\"\"\n  \n  print('trying to read the data...')\n  \n  try:\n    # define the schema\n    schema = StructType([\n      StructField('crim',DoubleType(),True),\n      StructField('zn',DoubleType(),True),\n      StructField('indus',DoubleType(),True),\n      StructField('chas',IntegerType(),True),\n      StructField('nox',DoubleType(),True),\n      StructField('rm',DoubleType(),True),\n      StructField('age',DoubleType(),True),\n      StructField('dis',DoubleType(),True),\n      StructField('rad',IntegerType(),True),\n      StructField('tax',IntegerType(),True),\n      StructField('ptratio',DoubleType(),True),\n      StructField('b',DoubleType(),True),\n      StructField('lstat',DoubleType(),True),\n      StructField('medv',DoubleType(),True)]\n    )\n\n    df = (spark.read\n          .option(\"HEADER\", True)\n          .schema(schema)\n          .csv(datapath))\n    \n  except ValueError:\n    print('At least, one variable format is wrong! \\\n    Please check the data')\n      \n  else:\n    print('Data to score have been read successfully!')\n    return df\n  \n# #Preprocessing\n\n# def preprocessing(df):\n  \n#   \"\"\"\n#   Function to preprocess data \n#   :param df: A pyspark DataFrame \n#   :return: abt_to_score\n#   \"\"\"\n  \n#   print('Data preprocessing...')\n  \n#   features = df.schema.names[:-1]\n#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n#   abt_to_score = assembler_features.transform(df)\n#   return abt_to_score\n\n# #Scoring\n# def score_data(abt_to_score, modelPath):\n  \n#   \"\"\"\n#   Function to score data \n#   :param abt_to_score: A pyspark DataFrame to score\n#   :param modelPath: The modelpath associated to .zip mleap flavor\n#   :return: scoredData\n#   \"\"\"\n  \n#   print('Scoring process starts...')\n  \n#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n#   scoredData = deserializedPipeline.transform(abt_to_score)\n#   return scoredData  \n  \n# def write_output_csv(scoredData, outputPath_CSV):\n  \n#   \"\"\"\n#   Function to write predictions\n#   :param scoredData: A pyspark DataFrame of predictions\n#   :param outputPath: The path to write the ouput table\n#   :return: scoredData\n#   \"\"\"\n\n#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n#   return outputDf.toPandas().to_dict()\n  \n# def main():\n\n#   parser = argparse.ArgumentParser(description='Score')\n\n#   parser.add_argument('-s', dest=\"Spark_Session\",\n#                       help='Provide the name of Spark Session')\n  \n#   parser.add_argument('-i', dest=\"inputpath_CSV\",\n#                         required=True, help='Provide the input path of data to score')\n\n#   args = parser.parse_args()\n#   spark_session = args.Spark_session\n#   input_path_CSV = args.Input_path_CSV\n  \n#   #Create a Spark Session\n#   spark = SparkSession.builder.appName(spark_session).getOrCreate()\n  \n#   #Read data\n#   read_data_csv(spark, inputPath_CSV)\n  \n  \n# if __name__==\"__main__\":\n#   sys.exit(main())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from click.testing import CliRunner\n\nrunner = CliRunner()\nresult1 = runner.invoke(read_data_csv, ['--datapath', '/data/boston_house_prices.csv'], catch_exceptions=True)\n\nassert result1.exit_code == 0, \"Code failed\" # Check to see that it worked\n\nprint(\"Success!\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print(result1.output)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["dbutils.fs.put(f\"{MLpackagePath}/score.py\", \n\n\"\"\"\n#!/usr/bin/python\n\nprint('suca')\n\n\"\"\".strip(), True)\n               "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["import subprocess\n\n# errors in the created process are raised here too\noutput = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\"], universal_newlines=True)\n\nprint(output)"],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"name":"2_Enginerring","notebookId":3075853380345907},"nbformat":4,"nbformat_minor":0}
