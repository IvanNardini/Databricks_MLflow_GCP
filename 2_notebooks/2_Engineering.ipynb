{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging Champion Model (Mlean Flavor) for GCP deployment\n",
    "\n",
    "Something to have in mind: \n",
    "\n",
    "Add the following jar files inside $SPARK_HOME/jars\n",
    "1. mleap-spark-base_2.11-0.7.0.jar\n",
    "2. mleap-core_2.11-0.7.0.jar\n",
    "3. mleap-runtime_2.11-0.7.0.jar\n",
    "4. mleap-spark_2.11-0.7.0.jar\n",
    "5. bundle-ml_2.11-0.7.0.jar\n",
    "6. config-0.3.0.jar\n",
    "7. scalapb-runtime_2.11-0.6.1.jar\n",
    "8. mleap-tensor_2.11-0.7.0.jar\n",
    "\n",
    "and then \n",
    "\n",
    "9. installed using pip mleap (0.7.0) - MLeap Python API\n",
    "\n",
    "### References\n",
    "\n",
    "- https://docs.azuredatabricks.net/_static/notebooks/mleap-model-export-demo-scala.html\n",
    "- https://github.com/combust/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree\n",
    "- https://github.com/combust/mleap/issues/172\n",
    "- https://cloud.google.com/dataproc/docs/tutorials/spark-scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple MLflow project programmatically with:\n",
    "\n",
    "1. Create a Working Dir\n",
    "\n",
    "2. Create a scala job\n",
    "\n",
    "2. Create score.py\n",
    "\n",
    "<!-- 3. Create the .sh to run the score.py\n",
    "\n",
    "\n",
    "2. Create the ML project:\n",
    "  - MLProject file\n",
    "  - Conda environment\n",
    "  - Basic machine learning script\n",
    "\n",
    "3. Create the scoring script\n",
    "4. Test the scoring script\n",
    "5. Create the entrypoint file:\n",
    "  - execute .sh (Create a Spark cluster, Install Mlflow, Run Batch Scoring Job based on score python code in cloud bucket) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context Web UI available at http://6f23552da701:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1587031675237).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "ml.combust.mleap#mleap-spark_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f163985b-3f0a-46ed-87fe-76debbf8df00;1.0\n",
      "\tconfs: [default]\n",
      "\tfound ml.combust.mleap#mleap-spark_2.11;0.15.0 in central\n",
      "\tfound ml.combust.mleap#mleap-spark-base_2.11;0.15.0 in central\n",
      "\tfound ml.combust.mleap#mleap-runtime_2.11;0.15.0 in central\n",
      "\tfound ml.combust.mleap#mleap-core_2.11;0.15.0 in central\n",
      "\tfound ml.combust.mleap#mleap-base_2.11;0.15.0 in central\n",
      "\tfound ml.combust.mleap#mleap-tensor_2.11;0.15.0 in central\n",
      "\tfound io.spray#spray-json_2.11;1.3.2 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound ml.combust.bundle#bundle-ml_2.11;0.15.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.5.1 in central\n",
      "\tfound com.thesamet.scalapb#scalapb-runtime_2.11;0.7.1 in central\n",
      "\tfound com.thesamet.scalapb#lenses_2.11;0.7.0-test2 in central\n",
      "\tfound com.lihaoyi#fastparse_2.11;1.0.0 in central\n",
      "\tfound com.lihaoyi#fastparse-utils_2.11;1.0.0 in central\n",
      "\tfound com.lihaoyi#sourcecode_2.11;0.1.4 in central\n",
      "\tfound com.jsuereth#scala-arm_2.11;2.0 in central\n",
      "\tfound com.typesafe#config;1.3.0 in central\n",
      "\tfound commons-io#commons-io;2.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      "\tfound ml.combust.bundle#bundle-hdfs_2.11;0.15.0 in central\n",
      ":: resolution report :: resolve 566ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.5.1 from central in [default]\n",
      "\tcom.jsuereth#scala-arm_2.11;2.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse-utils_2.11;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse_2.11;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#sourcecode_2.11;0.1.4 from central in [default]\n",
      "\tcom.thesamet.scalapb#lenses_2.11;0.7.0-test2 from central in [default]\n",
      "\tcom.thesamet.scalapb#scalapb-runtime_2.11;0.7.1 from central in [default]\n",
      "\tcom.typesafe#config;1.3.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.5 from central in [default]\n",
      "\tio.spray#spray-json_2.11;1.3.2 from central in [default]\n",
      "\tml.combust.bundle#bundle-hdfs_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.bundle#bundle-ml_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-base_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-core_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-runtime_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark-base_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark_2.11;0.15.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-tensor_2.11;0.15.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;3.5.0 by [com.google.protobuf#protobuf-java;3.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f163985b-3f0a-46ed-87fe-76debbf8df00\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/13ms)\n",
      "20/04/16 10:07:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# spark-shell --packages ml.combust.mleap:mleap-spark_2.11:0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mleap==0.15.0\n",
      "  Downloading mleap-0.15.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.0b1 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0) (1.18.1)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0) (1.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.dev0 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0) (0.22.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.18.1->mleap==0.15.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.18.1->mleap==0.15.0) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18.dev0->mleap==0.15.0) (0.14.1)\n",
      "Installing collected packages: mleap\n",
      "Successfully installed mleap-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mleap==0.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a working Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLpackagePath = \"/FileStore/ModelProjects/Boston_ML\"\n",
    "# dbutils.fs.rm(MLpackagePath, True)\n",
    "# dbutils.fs.mkdirs(MLpackagePath)\n",
    "# dbutils.fs.ls(MLpackagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the environment\n",
    "# # Copy data to score\n",
    "# dbutils.fs.cp(\"dbfs:/data/boston_house_prices.csv\", \"dbfs:/FileStore/ModelProjects/Boston_ML\")\n",
    "# # Copy model to consume for scoring\n",
    "# dbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"dbfs:/FileStore/ModelProjects/Boston_ML\")\n",
    "# # Check the content\n",
    "# dbutils.fs.ls(MLpackagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the folder programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy model to consume for scoring\n",
    "# dbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"/tmp\")\n",
    "# # Check the content\n",
    "# dbutils.fs.ls(\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create score.py job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import mleap.pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "\n",
    "def read_data_csv(spark, inputPath_CSV):\n",
    "    \n",
    "    '''\n",
    "    Function to load data in the Spark Session \n",
    "    :param spark: spark session \n",
    "    :param inputPath: path to get the data \n",
    "    :return: df\n",
    "    '''\n",
    "    \n",
    "    print('Trying to read the data...')\n",
    "    \n",
    "    try:\n",
    "        schema = StructType([\n",
    "          StructField('crim',DoubleType(),True),\n",
    "          StructField('zn',DoubleType(),True),\n",
    "          StructField('indus',DoubleType(),True),\n",
    "          StructField('chas',IntegerType(),True),\n",
    "          StructField('nox',DoubleType(),True),\n",
    "          StructField('rm',DoubleType(),True),\n",
    "          StructField('age',DoubleType(),True),\n",
    "          StructField('dis',DoubleType(),True),\n",
    "          StructField('rad',IntegerType(),True),\n",
    "          StructField('tax',IntegerType(),True),\n",
    "          StructField('ptratio',DoubleType(),True),\n",
    "          StructField('b',DoubleType(),True),\n",
    "          StructField('lstat',DoubleType(),True),\n",
    "          StructField('medv',DoubleType(),True)]\n",
    "        )\n",
    "        \n",
    "        df = (spark.read\n",
    "          .option(\"HEADER\", True)\n",
    "          .schema(schema)\n",
    "          .csv(inputPath_CSV))\n",
    "    \n",
    "    except ValueError:\n",
    "        print('At least, one variable format is wrong! Please check the data')\n",
    "      \n",
    "    else:\n",
    "        print('Data to score have been read successfully!')\n",
    "        return df\n",
    "\n",
    "def preprocessing(df):\n",
    "\n",
    "    '''\n",
    "    Function to preprocess data \n",
    "    :param df: A pyspark DataFrame \n",
    "    :return: abt_to_score\n",
    "    '''\n",
    "    \n",
    "    print('Data preprocessing...')\n",
    "\n",
    "    features = df.schema.names[:-1]\n",
    "    assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    abt_to_score = assembler_features.transform(df)\n",
    "    \n",
    "    print('Data have been processed successfully!')\n",
    "    return abt_to_score\n",
    "\n",
    "def score_data(abt_to_score, modelPath):\n",
    "    \n",
    "    '''\n",
    "    Function to score data \n",
    "    :param abt_to_score: A pyspark DataFrame to score\n",
    "    :param modelPath: The modelpath associated to .zip mleap flavor\n",
    "    :return: scoredData\n",
    "    '''\n",
    "    print('Scoring process starts...')\n",
    "    \n",
    "    deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelPath))\n",
    "    scoredData = deserializedPipeline.transform(abt_to_score)\n",
    "    return scoredData  \n",
    "  \n",
    "def write_output_csv(scoredData, outputPath_CSV):\n",
    "    '''\n",
    "    Function to write predictions\n",
    "    :param scoredData: A pyspark DataFrame of predictions\n",
    "    :param outputPath: The path to write the ouput table\n",
    "    :return: scoredData\n",
    "    '''\n",
    "    print('Writing Prediction in {}'.format(outputPath_CSV))\n",
    "    scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n",
    "    return scoredData.toPandas().to_dict()\n",
    "\n",
    "def evaluator(predictions):\n",
    "    \n",
    "    '''\n",
    "    Function to produce some evaluation stats\n",
    "    :param predictions: A pyspark DataFrame of predictions\n",
    "    :return: rmse, mse, r2, mae\n",
    "    '''\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"medv\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    \n",
    "    return rmse, mse, r2, mae\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Score')\n",
    "    \n",
    "    parser.add_argument('--input', dest=\"inputpath_CSV\",\n",
    "                        required=True, help='Provide the input path of data to score')\n",
    "    \n",
    "    parser.add_argument('--model', dest=\"modelPath\",\n",
    "                        required=True, help='Provide the model path to score')\n",
    "    \n",
    "    parser.add_argument('--output', dest=\"outputpath_CSV\",\n",
    "                        required=True, help='Provide the model path to score')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    input_path_CSV = args.inputpath_CSV\n",
    "    modelPath = args.modelPath\n",
    "    output_path_CSV = args.outputpath_CSV\n",
    "  \n",
    "    try:\n",
    "#         spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .master(SPARK_MASTER) \\\n",
    "#         .config('spark.executor.memory', TOTAL_MEMORY) \\\n",
    "#         .config('spark.cores.max', TOTAL_CORES) \\\n",
    "#         .config('spark.jars.packages',\n",
    "#                 'ml.combust.mleap:mleap-spark-base_2.11:0.9.3,ml.combust.mleap:mleap-spark_2.11:0.9.3') \\\n",
    "#         .appName(\"ClassifierTraining\") \\\n",
    "#         .getOrCreate()\n",
    "        spark = SparkSession.builder.appName('MyApp').getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"OFF\")\n",
    "        print('Created a SparkSession')\n",
    "    \n",
    "    except ValueError:\n",
    "        warnings.warn('Check')\n",
    "  \n",
    "    #Read data\n",
    "    data_to_process = read_data_csv(spark, input_path_CSV)\n",
    "    #Preprocessing\n",
    "    abt = preprocessing(data_to_process)\n",
    "    #Scoring\n",
    "    abt_scored = score_data(abt, modelPath)\n",
    "    #Write data\n",
    "    write_output_csv(abt_scored, output_path_CSV)\n",
    "    #Evaluate Model\n",
    "    evalstats = evaluator(abt_scored)\n",
    "    return evalstats\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    stats = main()\n",
    "    print('-'*20)\n",
    "    print('Process Log')\n",
    "    print('-'*20)\n",
    "    print('Scoring Job ends successfully!')\n",
    "    print(\"RMSE for the model: {}\".format(stats[0]))\n",
    "    print(\"MSE for the model: {}\".format(stats[1]))\n",
    "    print(\"R2 for the model: {}\".format(stats[2]))\n",
    "    print(\"MAE for the model: {}\".format(stats[3]))\n",
    "    print('Look at the Storage Bucket to get predictions!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Trying to read the data...\n",
      "Data to score have been read successfully!\n",
      "Data preprocessing...\n",
      "Data have been processed successfully!\n",
      "Scoring process starts...\n",
      "Writing Prediction in /home/jovyan/work/1_data/boston_house_prices_scored.csv\n",
      "--------------------\n",
      "Process Log\n",
      "--------------------\n",
      "Scoring Job ends successfully!\n",
      "RMSE for the model: 4.696684029858866\n",
      "MSE for the model: 22.05884087633132\n",
      "R2 for the model: 0.7386998714429953\n",
      "MAE for the model: 3.3284024432759862\n",
      "Look at the Storage Bucket to get predictions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/04/16 12:20:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python score.py --input \"/home/jovyan/work/1_data/boston_house_prices.csv\" \\\n",
    "    --model \"/home/jovyan/work/2_notebooks/output/ModelProjects_Boston_ML_lrModel.zip\"\\\n",
    "    --output  \"/home/jovyan/work/1_data/boston_house_prices_scored.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Create a Spark Scala job to run on Cloud Dataproc for deploying the model in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.put(f\"{MLpackagePath}/score.py\",\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "# Read Data\n",
    "\n",
    "# def read_data_csv(spark, inputPath_CSV):\n",
    "  \n",
    "#   '''\n",
    "#   Function to load data in the Spark Session \n",
    "#   :param spark: spark session \n",
    "#   :param inputPath: path to get the data \n",
    "#   :return: df\n",
    "#   '''\n",
    "  \n",
    "#   print('trying to read the data...')\n",
    "  \n",
    "#   try:\n",
    "#     # define the schema\n",
    "#     schema = StructType([\n",
    "#       StructField('crim',DoubleType(),True),\n",
    "#       StructField('zn',DoubleType(),True),\n",
    "#       StructField('indus',DoubleType(),True),\n",
    "#       StructField('chas',IntegerType(),True),\n",
    "#       StructField('nox',DoubleType(),True),\n",
    "#       StructField('rm',DoubleType(),True),\n",
    "#       StructField('age',DoubleType(),True),\n",
    "#       StructField('dis',DoubleType(),True),\n",
    "#       StructField('rad',IntegerType(),True),\n",
    "#       StructField('tax',IntegerType(),True),\n",
    "#       StructField('ptratio',DoubleType(),True),\n",
    "#       StructField('b',DoubleType(),True),\n",
    "#       StructField('lstat',DoubleType(),True),\n",
    "#       StructField('medv',DoubleType(),True)]\n",
    "#     )\n",
    "\n",
    "#     df = (spark.read\n",
    "#           .option(\"HEADER\", True)\n",
    "#           .schema(schema)\n",
    "#           .csv(datapath))\n",
    "    \n",
    "#   except ValueError:\n",
    "#     print('At least, one variable format is wrong! \\\n",
    "#     Please check the data')\n",
    "      \n",
    "#   else:\n",
    "#     print('Data to score have been read successfully!')\n",
    "#     return df\n",
    "  \n",
    "# #Preprocessing\n",
    "\n",
    "# def preprocessing(df):\n",
    "  \n",
    "#   '''\n",
    "#   Function to preprocess data \n",
    "#   :param df: A pyspark DataFrame \n",
    "#   :return: abt_to_score\n",
    "#   '''\n",
    "  \n",
    "#   print('Data preprocessing...')\n",
    "  \n",
    "#   features = df.schema.names[:-1]\n",
    "#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "#   abt_to_score = assembler_features.transform(df)\n",
    "#   return abt_to_score\n",
    "\n",
    "# #Scoring\n",
    "# def score_data(abt_to_score, modelPath):\n",
    "  \n",
    "#   '''\n",
    "#   Function to score data \n",
    "#   :param abt_to_score: A pyspark DataFrame to score\n",
    "#   :param modelPath: The modelpath associated to .zip mleap flavor\n",
    "#   :return: scoredData\n",
    "#   '''\n",
    "  \n",
    "#   print('Scoring process starts...')\n",
    "  \n",
    "#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n",
    "#   scoredData = deserializedPipeline.transform(abt_to_score)\n",
    "#   return scoredData  \n",
    "  \n",
    "# def write_output_csv(scoredData, outputPath_CSV):\n",
    "  \n",
    "#   '''\n",
    "#   Function to write predictions\n",
    "#   :param scoredData: A pyspark DataFrame of predictions\n",
    "#   :param outputPath: The path to write the ouput table\n",
    "#   :return: scoredData\n",
    "#   '''\n",
    "\n",
    "#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n",
    "#   return outputDf.toPandas().to_dict()\n",
    "  \n",
    "def main():\n",
    "\n",
    "  parser = argparse.ArgumentParser(description='Score')\n",
    "  \n",
    "  parser.add_argument('-i', dest=\"inputpath_CSV\",\n",
    "                        required=True, help='Provide the input path of data to score')\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  input_path_CSV = args.inputpath_CSV\n",
    "  \n",
    "#   #Create a Spark Session\n",
    "#   spark = SparkSession.builder.appName('MyApp').config(\"spark.master\", \"local\").getOrCreate()\n",
    "  \n",
    "  #Read data\n",
    "  #read_data_csv(spark, input_path_CSV)\n",
    "  \n",
    "  \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "  from pyspark import SparkContext\n",
    "  from pyspark.sql import SQLContext\n",
    "\n",
    "  try:\n",
    "    conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"My app\")\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "   # sqlContext = SQLContext.getOrCreate(sc)\n",
    "#     sc = pyspark.SparkContext()\n",
    "#     sc.setLogLevel(\"ERROR\")\n",
    "#     sqlContext = pyspark.sql.SQLContext(sc)\n",
    "    print('Created a SparkContext')\n",
    "      \n",
    "  except ValueError:\n",
    "      warnings.warn('SparkContext already exists in this scope')\n",
    "      \n",
    "  sys.exit(main())\n",
    "\n",
    "\"\"\".strip(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# errors in the created process are raised here too\n",
    "try:\n",
    "  output = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\", \"-i\", \"/dbfs/FileStore/ModelProjects/Boston_ML/boston_house_prices.csv\"], stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "except subprocess.CalledProcessError as exc:\n",
    "    print(\"Status : FAIL\", exc.returncode, exc.output)\n",
    "else:\n",
    "    print(\"Output: \\n{}\\n\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import click\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--inputPath_CSV\", type=str, )\n",
    "\n",
    "# Read Data\n",
    "\n",
    "def read_data_csv(spark, inputPath_CSV):\n",
    "  \n",
    "  \"\"\"\n",
    "  Function to load data in the Spark Session \n",
    "  :param spark: spark session \n",
    "  :param inputPath: path to get the data \n",
    "  :return: df\n",
    "  \"\"\"\n",
    "  \n",
    "  print('trying to read the data...')\n",
    "  \n",
    "  try:\n",
    "    # define the schema\n",
    "    schema = StructType([\n",
    "      StructField('crim',DoubleType(),True),\n",
    "      StructField('zn',DoubleType(),True),\n",
    "      StructField('indus',DoubleType(),True),\n",
    "      StructField('chas',IntegerType(),True),\n",
    "      StructField('nox',DoubleType(),True),\n",
    "      StructField('rm',DoubleType(),True),\n",
    "      StructField('age',DoubleType(),True),\n",
    "      StructField('dis',DoubleType(),True),\n",
    "      StructField('rad',IntegerType(),True),\n",
    "      StructField('tax',IntegerType(),True),\n",
    "      StructField('ptratio',DoubleType(),True),\n",
    "      StructField('b',DoubleType(),True),\n",
    "      StructField('lstat',DoubleType(),True),\n",
    "      StructField('medv',DoubleType(),True)]\n",
    "    )\n",
    "\n",
    "    df = (spark.read\n",
    "          .option(\"HEADER\", True)\n",
    "          .schema(schema)\n",
    "          .csv(datapath))\n",
    "    \n",
    "  except ValueError:\n",
    "    print('At least, one variable format is wrong! \\\n",
    "    Please check the data')\n",
    "      \n",
    "  else:\n",
    "    print('Data to score have been read successfully!')\n",
    "    return df\n",
    "  \n",
    "# #Preprocessing\n",
    "\n",
    "# def preprocessing(df):\n",
    "  \n",
    "#   \"\"\"\n",
    "#   Function to preprocess data \n",
    "#   :param df: A pyspark DataFrame \n",
    "#   :return: abt_to_score\n",
    "#   \"\"\"\n",
    "  \n",
    "#   print('Data preprocessing...')\n",
    "  \n",
    "#   features = df.schema.names[:-1]\n",
    "#   assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "#   abt_to_score = assembler_features.transform(df)\n",
    "#   return abt_to_score\n",
    "\n",
    "# #Scoring\n",
    "# def score_data(abt_to_score, modelPath):\n",
    "  \n",
    "#   \"\"\"\n",
    "#   Function to score data \n",
    "#   :param abt_to_score: A pyspark DataFrame to score\n",
    "#   :param modelPath: The modelpath associated to .zip mleap flavor\n",
    "#   :return: scoredData\n",
    "#   \"\"\"\n",
    "  \n",
    "#   print('Scoring process starts...')\n",
    "  \n",
    "#   deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelpath))\n",
    "#   scoredData = deserializedPipeline.transform(abt_to_score)\n",
    "#   return scoredData  \n",
    "  \n",
    "# def write_output_csv(scoredData, outputPath_CSV):\n",
    "  \n",
    "#   \"\"\"\n",
    "#   Function to write predictions\n",
    "#   :param scoredData: A pyspark DataFrame of predictions\n",
    "#   :param outputPath: The path to write the ouput table\n",
    "#   :return: scoredData\n",
    "#   \"\"\"\n",
    "\n",
    "#   scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n",
    "#   return outputDf.toPandas().to_dict()\n",
    "  \n",
    "# def main():\n",
    "\n",
    "#   parser = argparse.ArgumentParser(description='Score')\n",
    "\n",
    "#   parser.add_argument('-s', dest=\"Spark_Session\",\n",
    "#                       help='Provide the name of Spark Session')\n",
    "  \n",
    "#   parser.add_argument('-i', dest=\"inputpath_CSV\",\n",
    "#                         required=True, help='Provide the input path of data to score')\n",
    "\n",
    "#   args = parser.parse_args()\n",
    "#   spark_session = args.Spark_session\n",
    "#   input_path_CSV = args.Input_path_CSV\n",
    "  \n",
    "#   #Create a Spark Session\n",
    "#   spark = SparkSession.builder.appName(spark_session).getOrCreate()\n",
    "  \n",
    "#   #Read data\n",
    "#   read_data_csv(spark, inputPath_CSV)\n",
    "  \n",
    "# if __name__==\"__main__\":\n",
    "#   sys.exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from click.testing import CliRunner\n",
    "\n",
    "runner = CliRunner()\n",
    "result1 = runner.invoke(read_data_csv, ['--datapath', '/data/boston_house_prices.csv'], catch_exceptions=True)\n",
    "\n",
    "assert result1.exit_code == 0, \"Code failed\" # Check to see that it worked\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.put(f\"{MLpackagePath}/score.py\", \n",
    "\n",
    "\"\"\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "print('suca')\n",
    "\n",
    "\"\"\".strip(), True)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# errors in the created process are raised here too\n",
    "output = subprocess.check_output([\"python\",\"/dbfs/FileStore/ModelProjects/Boston_ML/score.py\"], universal_newlines=True)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "2_Enginerring",
  "notebookId": 3075853380345907
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
