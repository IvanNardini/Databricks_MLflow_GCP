{"nbformat_minor": 4, "cells": [{"source": "# Packaging Champion Model (Mlean Flavor) for GCP deployment\n\nThis notebook walks through the process of enginner and testing the PySpark job on Cloud Dataproc for deploying the model in batch\n\n#### Author: \n\n**Nardini, Ivan - Sr. Customer Advisor | CI & Analytics Team | ModelOps & Decisioning**\n\n## Setup\n\nAbout the setup:\n\n**1. Clone the git repo and set the environment**\n\n**2. Create pyspark job for scoring: score.py**\n\n**3. Test the script**", "cell_type": "markdown", "metadata": {}}, {"source": "## 1. Clone the git repo and set the environment\n\nMleap needs jar files (inside SPARK_HOME/jars). \n\nSome of them are:\n\n1. mleap-spark-base_xxx.jar\n2. mleap-core_xxx.jar\n3. mleap-runtime_xxx.jar\n4. mleap-spark_xxx.jar\n5. bundle-ml_xxx.jar\n6. config-0.3.0.jar\n7. scalapb-runtime_xxx.jar\n8. mleap-tensor_xxx.jar\n\nand then installed using pip mleap - MLeap Python API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\ncd home\ngit clone https://github.com/IvanNardini/Databricks_MLflow_GCP\ncp ./Databricks_MLflow_GCP/2_notebooks/output/ModelProjects_Boston_ML_lrModel.zip /tmp/model.zip", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\npip freeze", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\npip install mleap==0.15.0\npip install pyspark==2.4.5", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\nspark-shell --packages ml.combust.mleap:mleap-spark_2.11:0.15.0", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\ncp -r /root/.ivy2/jars/*.jar /usr/lib/spark/jars/", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!ls -la /usr/lib/spark/jars/", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Restart Kernel\n# import os\n# os._exit(00)", "outputs": [], "metadata": {}}, {"source": "## Create Pyspark job (score.py) for scoring", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "os.chdir('./Databricks_MLflow_GCP/2_notebooks/output/')\nos.getcwd()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile score.py\n\n#!/usr/bin/python\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\nfrom pyspark.ml.feature import VectorAssembler\nimport mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nimport os\nimport sys\nimport argparse\nimport tempfile\nimport warnings\n\n\ndef read_data_csv(spark, inputPath_CSV):\n    \n    '''\n    Function to load data in the Spark Session \n    :param spark: spark session \n    :param inputPath: path to get the data \n    :return: df\n    '''\n    \n    print('Trying to read the data...')\n    \n    try:\n        schema = StructType([\n          StructField('crim',DoubleType(),True),\n          StructField('zn',DoubleType(),True),\n          StructField('indus',DoubleType(),True),\n          StructField('chas',IntegerType(),True),\n          StructField('nox',DoubleType(),True),\n          StructField('rm',DoubleType(),True),\n          StructField('age',DoubleType(),True),\n          StructField('dis',DoubleType(),True),\n          StructField('rad',IntegerType(),True),\n          StructField('tax',IntegerType(),True),\n          StructField('ptratio',DoubleType(),True),\n          StructField('b',DoubleType(),True),\n          StructField('lstat',DoubleType(),True),\n          StructField('medv',DoubleType(),True)]\n        )\n        \n        df = (spark.read\n          .option(\"HEADER\", True)\n          .schema(schema)\n          .csv(inputPath_CSV))\n    \n    except ValueError:\n        print('At least, one variable format is wrong! Please check the data')\n      \n    else:\n        print('Data to score have been read successfully!')\n        return df\n\ndef preprocessing(df):\n\n    '''\n    Function to preprocess data \n    :param df: A pyspark DataFrame \n    :return: abt_to_score\n    '''\n    \n    print('Data preprocessing...')\n\n    features = df.schema.names[:-1]\n    assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n    abt_to_score = assembler_features.transform(df)\n    \n    print('Data have been processed successfully!')\n    return abt_to_score\n\ndef score_data(abt_to_score, modelPath):\n    \n    '''\n    Function to score data \n    :param abt_to_score: A pyspark DataFrame to score\n    :param modelPath: The modelpath associated to .zip mleap flavor\n    :return: scoredData\n    '''\n    print('Scoring process starts...')\n    \n    deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:{}\".format(modelPath))\n    scoredData = deserializedPipeline.transform(abt_to_score)\n    return scoredData  \n  \ndef write_output_csv(scoredData, outputPath_CSV):\n    '''\n    Function to write predictions\n    :param scoredData: A pyspark DataFrame of predictions\n    :param outputPath: The path to write the ouput table\n    :return: scoredData\n    '''\n    print('Writing Prediction in {}'.format(outputPath_CSV))\n    scoredData.toPandas().to_csv(outputPath_CSV, sep=',', index=False)\n    return scoredData.toPandas().to_dict()\n\ndef evaluator(predictions):\n    \n    '''\n    Function to produce some evaluation stats\n    :param predictions: A pyspark DataFrame of predictions\n    :return: rmse, mse, r2, mae\n    '''\n    evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"medv\")\n    rmse = evaluator.evaluate(predictions)\n    mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n    \n    return rmse, mse, r2, mae\n\ndef main():\n    \n    parser = argparse.ArgumentParser(description='Score')\n    \n    parser.add_argument('--input', dest=\"inputpath_CSV\",\n                        required=True, help='Provide the input path of data to score')\n\n    #Mleap deserializeFromBundle method does not work with URL on GCP\n    \n    # parser.add_argument('--model', dest=\"modelPath\",\n    #                     required=True, help='Provide the model path to score')\n    \n    parser.add_argument('--output', dest=\"outputpath_CSV\",\n                        required=True, help='Provide the model path to score')\n\n    args = parser.parse_args()\n    input_path_CSV = args.inputpath_CSV\n    # modelPath = args.modelPath\n    modelPath = '/tmp/model.zip'\n    output_path_CSV = args.outputpath_CSV\n  \n    try:\n#         spark = SparkSession \\\n#         .builder \\\n#         .master() \\\n#         .config('spark.jars.packages',\n#                 'ml.combust.mleap:mleap-spark-base_2.11:0.15.0,ml.combust.mleap:mleap-spark_2.11:0.15.0') \\\n#         .appName(\"RegressionScoring\") \\\n#         .getOrCreate()\n        spark = SparkSession.builder.appName('RegressionScoring').getOrCreate()\n        spark.sparkContext.setLogLevel(\"OFF\")\n        print('Created a SparkSession')\n    \n    except ValueError:\n        warnings.warn('Check')\n  \n    #Read data\n    data_to_process = read_data_csv(spark, input_path_CSV)\n    #Preprocessing\n    abt = preprocessing(data_to_process)\n    #Scoring\n    abt_scored = score_data(abt, modelPath)\n    #Write data\n    write_output_csv(abt_scored, output_path_CSV)\n    #Evaluate Model\n    evalstats = evaluator(abt_scored)\n    return evalstats\n    \n    \nif __name__==\"__main__\":\n    \n    stats = main()\n    print('-'*20)\n    print('Process Log')\n    print('-'*20)\n    print('Scoring Job ends successfully!')\n    print(\"RMSE for the model: {}\".format(stats[0]))\n    print(\"MSE for the model: {}\".format(stats[1]))\n    print(\"R2 for the model: {}\".format(stats[2]))\n    print(\"MAE for the model: {}\".format(stats[3]))\n    print('Look at the Storage Bucket to get predictions!')\n    ", "outputs": [], "metadata": {}}, {"source": "### Test score.py", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\npython score.py --input \"/home/Databricks_MLflow_GCP/1_data/boston_house_prices.csv\" \\\n    --output  \"/home/Databricks_MLflow_GCP/1_data/boston_house_prices_scored.csv\" ", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "name": "2_Enginerring", "notebookId": 3075853380345907}}